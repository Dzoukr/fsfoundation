%% For double-blind review submission, w/o CCS and ACM Reference (max submission space)
%\documentclass[acmsmall,review,anonymous]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For double-blind review submission, w/ CCS and ACM Reference
%\documentclass[acmsmall,review,anonymous]{acmart}\settopmatter{printfolios=true}
%% For single-blind review submission, w/o CCS and ACM Reference (max submission space)
\documentclass[acmsmall,review]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For single-blind review submission, w/ CCS and ACM Reference
%\documentclass[acmsmall,review]{acmart}\settopmatter{printfolios=true}
%% For final camera-ready submission, w/ required CCS and ACM Reference
%\documentclass[acmsmall]{acmart}\settopmatter{}

\usepackage{listings,lipsum}

\lstnewenvironment{verbquote}[1][]
  {\lstset{columns=fullflexible,
           basicstyle=\ttfamily\small\it,
           xleftmargin=2em,
           xrightmargin=2em,
           breaklines,
           breakindent=0pt,
           #1}}% \begin{verbquote}[..]
  {}% \end{verbquote}

%% Journal information
%% Supplied to authors by publisher for camera-ready submission;
%% use defaults for review submission.
\acmJournal{PACMPL}
\acmVolume{1}
\acmNumber{HOPL} % CONF = POPL or ICFP or OOPSLA
\acmArticle{1}
\acmYear{2018}
\acmMonth{1}
\acmDOI{} % \acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}

%% Copyright information
%% Supplied to authors (based on authors' rights management selection;
%% see authors.acm.org) by publisher for camera-ready submission;
%% use 'none' for review submission.
\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\copyrightyear{2018}           %% If different from \acmYear

%% Bibliography style
\bibliographystyle{ACM-Reference-Format}
%% Citation style
%% Note: author/year citations are required for papers published as an
%% issue of PACMPL.
\citestyle{acmauthoryear}   %% For author/year citations


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Note: Authors migrating a paper from PACMPL format to traditional
%% SIGPLAN proceedings format must update the '\documentclass' and
%% topmatter commands above; see 'acmart-sigplanproc-template.tex'.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% Some recommended packages.
\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption


\begin{document}

%% Title information
\title{The Early History of F\#}         %% [Short Title] is optional;
                                        %% when present, will be used in
                                        %% header instead of Full Title.
%\titlenote{with title note}             %% \titlenote is optional;
                                        %% can be repeated if necessary;
                                        %% contents suppressed with 'anonymous'
%\subtitle{Subtitle}                     %% \subtitle is optional
%\subtitlenote{with subtitle note}       %% \subtitlenote is optional;
                                        %% can be repeated if necessary;
                                        %% contents suppressed with 'anonymous'


%% Author information
%% Contents and number of authors suppressed with 'anonymous'.
%% Each author should be introduced by \author, followed by
%% \authornote (optional), \orcid (optional), \affiliation, and
%% \email.
%% An author may have multiple affiliations and/or emails; repeat the
%% appropriate command.
%% Many elements are not rendered, but should be provided for metadata
%% extraction tools.

%% Author with single affiliation.
\author{Don Syme}
                                        %% can be repeated if necessary
%\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  \position{Principal Researcher}          %% \authornote is optional;
  %\department{Department1}              %% \department is recommended
  \institution{Microsoft}            %% \institution is required
  %\streetaddress{Street1 Address1}
  %\city{City1}
  %\state{State1}
  %\postcode{Post-Code1}
  \country{United Kingdom}                    %% \country is recommended
}
\email{don.syme@microsoft.com}          %% \email is recommended
\affiliation{
  \position{Technical Advisor}          %% \authornote is optional;
  %\department{Department1}              %% \department is recommended
  \institution{F\# Software Foundation}            %% \institution is required
  %\streetaddress{Street1 Address1}
  %\city{City1}
  %\state{State1}
  %\postcode{Post-Code1}
  %\country{United Kingdom}                    %% \country is recommended
}
%\email{don.syme@microsoft.com}          %% \email is recommended

%% Author with two affiliations and emails.
%\author{First2 Last2}
%\authornote{with author2 note}          %% \authornote is optional;
                                        %% can be repeated if necessary
%\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
%\affiliation{
 % \position{Position2a}
  %\department{Department2a}             %% \department is recommended
  %\institution{Institution2a}           %% \institution is required
  %\streetaddress{Street2a Address2a}
  %\city{City2a}
  %\state{State2a}
  %\postcode{Post-Code2a}
  %\country{Country2a}                   %% \country is recommended
%}
%\email{first2.last2@inst2a.com}         %% \email is recommended
%\affiliation{
  %\position{Position2b}
  %\department{Department2b}             %% \department is recommended
  %\institution{Institution2b}           %% \institution is required
  %\streetaddress{Street3b Address2b}
%  \city{City2b}
 % \state{State2b}
 % \postcode{Post-Code2b}
 % \country{Country2b}                   %% \country is recommended
%}
%\email{first2.last2@inst2b.org}         %% \email is recommended


%% Abstract
%% Note: \begin{abstract}...\end{abstract} environment must come
%% before \maketitle command
\begin{abstract}
This paper describes the genesis and early history of the F\# programming language. I start with the origins of strongly-typed functional programming (FP) in the 1970s, 80s and 90s. During the same period, Microsoft was founded and grew to dominate the software industry. In 1997, as a response to Java, Microsoft initiated internal projects which eventually became the .NET programming framework and the C\# language. From 1997 the worlds of academic functional programming and industry combined at Microsoft Research, Cambridge. The researchers engaged with the company through Project 7, the initial effort to bring multiple languages to .NET, leading to the initiation of1 .NET Generics in 1998 and F\# in 2002. F\# was one of several responses by advocates of strongly-typed functional programming to the "object-oriented tidal wave" of the mid-1990s. The development of the core features of F\# happened from 2004-2007, and I describe the decision-making process that led to the "productization" of F\# by Microsoft in 2007-10 and the release of F\# 2.0.  The origins of F\#'s characteristic features are covered: object programming, quotations, statically resolved type parameters, active patterns, computation expressions, async, units-of-measure and type providers. I describe key developments in F\# since 2010, including F\# 3.0-4.5, and its evolution as an open source, cross-platform language with multiple delivery channels.  I conclude by examining some uses of  F\# and the influence F\# has had on other languages so far.
\end{abstract}


%% 2012 ACM Computing Classification System (CSS) concepts
%% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011007.10011006.10011008</concept_id>
<concept_desc>Software and its engineering~General programming languages</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003456.10003457.10003521.10003525</concept_id>
<concept_desc>Social and professional topics~History of programming languages</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~General programming languages}
\ccsdesc[300]{Social and professional topics~History of programming languages}
%% End of generated code


%% Keywords
%% comma separated list
\keywords{Functional Programming, Programming Languages, F\#}  %% \keywords are mandatory in final camera-ready submission


%% \maketitle
%% Note: \maketitle command must come after title commands, author
%% commands, abstract environment, Computing Classification System
%% environment and commands, and keywords command.
\maketitle


\section*{Introduction}

The history of the F\# programming language is an arc drawn from the 1970s to the present day.  Somewhere, back in the early 1970s, an idea was born in the mind of Robin Milner and his colleagues Lockwood Morris and Malcolm Newey of a succinct, fully type-inferred functional programming language suitable for manipulating structured information. (Gordon, 2000).  Building on the tradition of LISP (and indeed using LISP as their implementation vehicle), this language became ML – Meta Language - and is the root of a tradition of “strongly typed functional programming languages” that includes Edinburgh ML, Miranda, Haskell, Standard ML, OCaml, Elm, ReasonML and PureScript. F\# is part of this family.

The history of Standard ML has been told elsewhere (REF: MacQueen, 2015). ML-family languages are often associated with formalism, a theme I explore later in this article. However, a primary concern of Milner and co. from the outset was pragmatic usability. This group needed their language for a specific purpose: to succinctly and accurately program the proof rules and transformations (“tactics”) of a theorem proving system called LCF, at that time on PDP-10 machines. Pragmatic choices included the use of mutable state (to allow proof state to be stored in an interactive system) and a type inference system (later called Hindley-Milner or Damas-Milner type inference), allowing the code for derived tactics to be both succinct and automatically generalized.  A similar theme of pragmatism ran through later ML dialects as well, including OCaml (REF: Leroy), witnessed by both the language design and tooling such as the OCaml C Foreign Function Interface (FFI).

Rolling forward, to the present day, key ideas stemming from the 1970s are at the core of the F\# language design and central to the day-to-day experience of using the language.  Like all ML-family languages:
\begin{itemize}
\item The core paradigm supported by F\# is still strongly typed functional programming;
\item The core activity of F\# is still defining types (\texttt{type X}) and functions (\texttt{let f x = ...}) and these declarations are type-inferred and generalized automatically;
\item F\# still aims to support a mode of programming where the focus is on the domain being manipulated rather than on the details of programming itself.
\end{itemize}

Today, books are published which extoll the virtues of F\# for “Domain Driven Design” (REF: Wlaschin, 2017).  This is not so far removed from the early role of ML where the “domain” was the symbolic representation of terms and theorems of the LCF logic. The “spirit” of ML is very much alive in F\#, as it was always intended to be. 

The leap from 1970s to the present day spans a period of massive change in the computing industry: we have shifted from PDP-10s to cloud systems, from punch cards to mobile phones, from edit-line to tooling-rich IDEs, from small to massive storage, from no-network to ubiquitous network. This article tells the story of how F\# developed, the industry and academic contexts in which this occurred, the immediate influences on the language and its distinctive contributions. The story intersects with many other histories in programming language design, including the complex histories of functional programming, object-oriented programming, type systems, runtime design, operating systems and open source software, and emphasis is placed on the genesis of F\# as one of several reactions to the “object-oriented tidal wave” of the early 1990s.  The story is necessarily incomplete and told largely from the personal perspective of the author, the designer of the language, and I apologize for that. Where references are not provided the text is offered as source material based on the recollection of the author.

I have started with the core idea of the ML-family of programming languages: type-safe, succinct, accurate, domain-oriented functional programming. From my perspective, this idea has “held strong, held true” throughout this era of change. Whether that is through obstinacy, coherence or coincidence is something I leave the reader to assess. 

\section*{What is F\# in 2020?}

In 2020, F\# is described on its documentation pages as “a functional programming language that runs on .NET.” The F\# language guide (Microsoft, 2019) calls out the following major features of the language, providing useful clarity about what the language is today:
\begin{itemize}
\item functions and modules
\item pipelines and composition
\item lists, arrays, sequences
\item pattern matching 
\item active patterns 
\item type inference
\item recursive functions
\item quotations
\item record types, discriminated union types
\item option types
\item units of measure
\item object programming
\item asynchronous programming
\item computation expressions
\item type providers
\end{itemize}
The documentation continues with an explanation of the main tooling and libraries available for F\# programming, including
\begin{itemize}
\item cross-platform compilation and execution;
\item the primary F\# and .NET libraries;
\item web, mobile and data programming toolkits;
\item editing tools from Emacs to Visual Studio, VS Code and Jet Brains Rider (TM);
\item how to use F\# with the cloud platform of the company providing the documentation.
\end{itemize}
Other resources for F\# follow a similar order of explanation, e.g. Fable  is a packaging of F\# for client-side web programming compiling to JavaScript (REF: Fable - The compiler that emits JavaScript you can be proud of! August 2019, Retrieved from http://fable.io ), and WebSharper (REF: Granicz, 2018) and SAFE-Stack  emphasize the use of F\# as a “full-stack” language where both client and server components are written in the same language (REF: SAFE Stack, An end-to-end functional-first stack for cloud-ready web development that emphasizes type-safe programming, August 2019,  Retrieved from https://safe-stack.github.io/)

That's what F\# is today: an open-source, cross-platform, strongly-typed, succinct programming language with broad applicability to many different programming scenarios and much loved by its users.  The language community centers around the F\# Software Foundation (FSSF, a US non-profit), and social media such as Twitter. F\# has had influence – most directly on C\# but also more broadly – I discuss this in the conclusion.  But how did we get there?

\section*{Background: Languages, Programmability at Microsoft and the Creation of .NET}

The 1970s-80s saw continual, rapid expansion of the computing industry, from transistor design and chip fabrication to software development and applications. Software development tooling both boomed and consolidated with the development and adoption of many different programming paradigms and languages, including BASIC, PASCAL, Prolog, Modula 2 and C.  Accompanying each were commercial variations (Visual Basic, Turbo Pascal, Borland C for example). Languages such as Logo served to spark the imagination of a new generation that programming could be “different”  and a bold new era of “fourth generation languages” was promised. 

At this time, Microsoft also saw massive expansion as an operating system and applications company.  Microsoft started by building programming tools in 1975 and the importance of programmability – both as a commercial and technical undertaking – was “in the bones” of the company and its CEO Bill Gates (Microsoft, 2012).  Through the 1980s his primary concern with regard to programmability was commercial: how to support the creation of applications and a commercial ecosystem of independent software vendors (ISVs) for the DOS and Windows ecosystems.  What mattered most was the sheer number of developers using these platforms, for developers would feed the growth of these ecosystems.  The company created tools such as Visual Basic to satisfy the mass-developer market, and versions of C for more hard-core developers, a distinction that later got characterized as tools for “Mort” (Visual Basic) and “Einstein” (C++) . Such tooling was pitted against a myriad of rapid development environments such as HyperCard  and ToolBook  and Microsoft succeeded hands-down, becoming dominant in application development worldwide and achieving a monopoly position in operating systems.  Microsoft also made numerous other programming tools including FoxPro  and a FORTRAN compiler , later discontinued.

The late 1980s saw a new wave of thinking coalesce around “object-oriented” programming, and this became increasingly influential in applied software development and academia.  Indeed, object-orientation moved from the margins to be central to the conceptualization of software development.  The pattern of languages with commercial toolchains repeated: examples include the first C++ commercial compilers in 1985, Borland C++ in 1992 and IBM Smalltalk in 1993.   Foremost amongst the drivers towards OO was the rising prevalence of user interface elements in software: applications were now interactive and made of “buttons” and other “widgets”, these widgets were easily conceptualized as “objects” combining state and behavior, and these widgets could be hierarchically classified. Procedurally-oriented languages were unable to express such abstractions directly in code, and languages without subtyping found it hard to express the necessary relationships between widgets. People assessed languages by asking two primary questions: “does it support inheritance?” and “is everything an object?”.  Any newly proposed language that did not meet these criteria quickly became marginalized into relative obscurity. 

The prospect of an industry-shifting nexus between this new wave of software development methodology and an operating system company drew tantalizingly near. For example, the launch of NEXTStep 3.0 in 1993 featured heavy focus on “objects” as a concept that the NEXTStep OS somehow supported (in practice this meant that NeXTStep application development was based on Objective C – the OS itself, was written in C). This was used by Jobs to demonstrate its sophistication and technical maturity. When Java was developed in 1991-95, and released in 1996, it was a deep challenge to Microsoft in at least six ways: 
\begin{itemize}
\item Java was object-oriented and “modern”;
\item Java promised Write Once Run Anywhere software development that could in theory cut the dependence on a particular operating system; 
\item Java was developed by a direct rival in the upper-end operating system market; 
\item Java was positioned as a web-technology at the dawn of the web, potentially capable of delivering end-user applications via the browser;
\item Java used a set of technical devices such as a virtual machine (VM) and garbage collection (GC);  and 
\item Java was recognized as a contribution to applied academic computer science , bringing on board a constituency who had been largely ignored by Microsoft. As a result, Java became embraced as a de-facto standard for typed object-orientation.
\end{itemize}

Microsoft was initially slow to respond.  Internally, the company was committed to C for implementing its flagship products but had plenty of assembly code as well. Given the target hardware specs it was unrealistic to write Windows or Microsoft Word in a heap-allocating “toy” language like Java, so Java was not going to become the major language of internal use at Microsoft quickly. Further, external-facing RAD environments like Visual Basic didn’t immediately benefit from the structured approach to OO found in class-oriented languages. With a tidal wave of Java hype flooding the industry, Microsoft responded by embracing Java, licensed from Sun in 1996 (Microsoft J++), but subsequently faced legal action for extending the language. This formed part of the background to United States v. Microsoft Corp, a legal case running from formally from 1998 to 2001 though with its roots in earlier actions. In this case the U.S. government accused Microsoft of illegally maintaining its monopoly position in the PC market, through restrictions on PC manufacturers relating to internet browser software and other programs such as Netscape and Java. The initial trial recommendations were to break-up Microsoft as a company, later settled to lesser remedies. 

In 1997, Microsoft changed tack and started the internal development of a new programmability platform which could address the fundamental challenge of Java, while also addressing the needs specific to Windows programmability.  Initially called COM+ 2.0 or Lightning, and eventually .NET, the founding principles of the runtime environment were as follows:
\begin{itemize}
\item It would support multiple programming languages, including Visual Basic, C++ and Java. Additionally, a new language was started, under the design of Anders Hejlsberg, initially called COOL and later C\#.
\item It would support a bytecode, garbage collection, JIT compilation and “middleware” features such as stack-based security checks and remoting. Additionally, the runtime would support unsigned integers, unboxed representations and install-time compilation.
\item It would be made specifically for application development on Windows, including native interoperability to C-based Win32 APIs and built-in support for COM. However, it would also be sufficiently general that porting to other operating systems would be theoretically possible.
\item Its SDK would be offered free and aligned with emerging efforts in academic relations, then managed by Microsoft Research, founded in 1992.
\end{itemize}

The decisions around Lightning were regularly reviewed by Bill Gates. Through the efforts of two “developer evangelists” - Peter and James Plamondon  – a key decision was made: Lightning would be a \emph{multi-language runtime} rather than just a fixed set of languages decided by Microsoft.  An outreach project called “Project 7” was initiated: the aim was to bring seven commercial languages and seven academic languages to target Lightning at launch. While in some ways this was a marketing activity, there was also serious belief and intent.  For help with defining the academic languages James Plamondon turned to Microsoft Research (MSR).

From the perspective of the history of F\#, this is a moment when largely unrelated traditions in the history of computer science began to merge and intertwine: the worlds of Robin Milner and Bill Gates began to meet.

MSR had been founded in 1992 and expanded to Cambridge UK in September 1997. Andy Gordon (a high-profile young researcher in programming language theory) and Luca Cardelli (author of one of the first ML implementations and prolific researcher) were hired, followed in September 1998 by Simon Peyton Jones (a leading Haskell contributor), Nick Benton (a theorist and initiator of MLj, discussed later), Cedric Fournet (a core member of the OCaml team), Sir Tony Hoare (world famous computer scientist) and Don Syme (the author of this paper; undergraduate student of early ML contributor Malcolm Newey in Australia; PhD student of Mike Gordon; with a background in functional programming, formal verification and Java). MSR eventually employed over 500 researchers and engineers in various locations.  

Suddenly Microsoft was brimming with academic computer scientists, though in a separate “org” to the “product teams”.  Many were eager to make an impact on Microsoft’s product range, and there was cultural memory from Bell Labs (Cardelli), DEC-SRC (Cardelli), Compaq (Gordon) and Intel (Syme) that this was how such labs “paid the bills”.  Each researcher was in their own way deeply evangelical about one point-of-view or another in computer science and often held tribal allegiances to their corresponding communities in academia, both of which shaped their interactions with product teams and the projects they chose. Many in the formal verification and theory areas had experience of strongly-typed functional programming. Robin Milner, the originator of the ML family of languages, was head of department at Cambridge University “across the road” and was held in high esteem as a pioneer in the field of research. 

On the other side, Microsoft was entering a phase where it was becoming deeply committed to a multi-language runtime and wanted to be seen to innovate positively.  Lightning already had many of the core elements of a typical functional language implementation (GC, JIT, bytecode), and promised to unite disparate themes in programming, though initially within the confines of the Windows operating system. The scene was set for interesting things to happen. The Lightning effort was renamed NGWS and then finally called .NET on launch in 2000.  

\section*{Background: Strongly Typed Functional Programming through the 1990s – Calculi, Miranda, OCaml, Haskell and Pizza}

While Microsoft was establishing its monopoly position in the early 1990s, and object-orientation was sweeping the globe, the world of strongly typed functional programming was small and marginalized yet active and vibrant.  This world overlapped with other fields of activity, which we would now call “PL research” but at the time included formal verification, type theory and programming logics and an increasing dose of category theory. This world was heavily influenced by foundational calculi, most obviously the Lambda Calculus and its variations such as System F, followed by concurrent calculi such as CCS and the Pi Calculus (REF: Sangiorgi \& Walker, 2001).  Efforts to identify unifying object calculi were well underway (REF: Cardelli \& Abadi, 1996) and workshops such as FOOL searched for foundational formalisms for new constructs being added to existing languages. 

“Formal methods” was an overlapping field in its hey-day in the 1980s, with major government initiatives in formalized hardware and software.  Controversies (MacKenzie, 2001) and the relatively modest successes of formal methods in industry saw researchers in the 1990s look to more pragmatic techniques for bug-finding including model checking and static analysis tools. Systems such as SMV, Z, ACL, HOL88, PVS, HOL90, Isabelle and commercial offerings were used to model, formalize and verify aspects of software and hardware designs.   Functional languages were often used to implement and script these systems, e.g. Edinburgh ML (HOL88), Standard ML (HOL90, Isabelle), OCaml (Coq, NuPRL), Caml Light (HOL-Lite), LISP (ACL2, PVS).  These systems thus formed a core constituency of adoption of strongly-typed functional languages and held functional programming close to more theoretical communities.  The Formal Definition of Standard ML (REF: Harper, Milner, \& Tofte, 1990) and its commentary were seen by some as almost holy texts, enshrining the virtues of standardization, cooperation, formalism and theory. At the same time, some functional programming systems were closely aligned to research on parallel programming, e.g. Parallel ML (REF: Rabhi \& Gorlatch, 2003) and parallel versions of Haskell.  Together these formed the context in which I first encountered strongly typed functional programming and ML in my undergraduate research work (Syme, 1993).

The FDIV bug at Intel, discovered in 1994,  led to a significant increase in formal verification investment on the part of hardware manufacturers. Intel turned to academia for help and among the projects brought in was Forte, led by Carl Seger, a toolchain using BDDs and theorem proving to verify the data paths of floating point circuits with respect to an IEEE model.  The Forte toolchain was built around a strongly typed functional language called Forte FL. Although not otherwise influential on programming language design, this is mentioned because I was employed as an intern on this project in 1996-97 and in this context experienced the extreme effectiveness of strongly-typed FP as a “glue language” for symbolic manipulations in applied formal verification, an early application domain for F\#. (Seger, et al., 2005). Forte FL also made many pragmatic choices, for example when interoperating with external data and the inclusion of quotations in the design of a strongly-typed language. This experience had significant impact on the later design of F\#.

Strongly-typed FP also saw significant use through Miranda, first released in 1985.  During the 1990s the small world of strongly-typed functional programming also split and diverged in ways typical of active research communities.  Haskell 98 united the streams of lazy, pure functional programming, precursors included HOPE and Miranda.  Standard ML from 1989 remained the unifying effort for mixed functional-imperative languages. However, the INRIA Project Cristal group saw the standardization as premature, and instead created Caml Light and then OCaml. (Leroy, 2019). Standard ML itself was heavily associated with its innovative module system and saw practical implementations in Poly ML, Standard ML of New Jersey and MLton.

Strongly-typed FP languages and compilers saw an ongoing trickle of interest, adoption and use. While not enough to challenge the massive adoption of C, C++ and Java, and largely unnoticed by industry, they were enough to sustain the languages, promote research and create small cohorts of dedicated advocates of OCaml, Standard ML and Haskell.   People who had the good fortune to use these languages in practice (including myself) experienced dramatic increases in productivity as well as some frustrations. As with the original ML implementation, the domain of use was typically symbolic programming of some kind. The experience of productivity was due to the peculiar effectiveness of the combination of features on offer: the “magic” of Hindley-Milner type inference to support safe, compositional programming; the effectiveness of parametric polymorphism (generics) and discriminated unions to describe and manipulate domain data; the correctness benefits of programming without pervasive null values; the close correspondence between code and formal models. These were in addition to the elegance and expressive power of expression-oriented programming, well-known from LISP but newly rediscovered with joy and delight by user after user. There was a strong feeling that these languages had the potential to be used much more broadly, and that valuable programming techniques were being lost through the widespread embrace of Java.  

The tidal wave of interest in object-orientation in the early 1990s had significant impact in academia, just as in industry.  By the mid-1990s many in the world of FP and PL were genuinely shocked, bewildered, disoriented and in some cases disillusioned by the rise of C++, Java and OO in general.  Reactions varied, and I now examine responses to the OO tidal wave that are key to understanding the genesis of F\#, Scala and other languages in the 2000s. 

One response to object-orientation was to “give in” and work on Java implementations. Others worked on formalisms around Java, and indeed I initially did just that for my PhD thesis (Syme, 1999) and others formulated and published foundational object calculi. Some responded by integrating object-oriented features into FP languages: LISP had already added CLOS, the Common LISP object system and OCaml saw the introduction of new forms of genericity (“row-polymorphism” and “column polymorphism”) used as the basis for a fascinating object system. (Garrigue, 2002).

Another response was to propose to integrate specific technical features associated with strongly-typed functional languages into “mainstream” OO languages.  Wadler and Odersky led the charge with the development of Pizza, a variation of Java that incorporated parametric polymorphism (generics), discriminated unions and first-class function values. (REF: Bracha, Odersky, Stoutamire, \& Wadler, 1998)  This was subsequently trimmed-down to the proposal Generic Java (GJ), and later heavily influenced C\#, Scala and F\#. Ultimately GJ became the basis for Java generics, though its use of “erasure” and lack of accurate runtime type information were significant compromises. 

An alternative angle was to “deconstruct” functional programming itself and examine the underlying problems (as exhibited by implementations of Haskell or Standard ML for example). One instance of this was the paper Why no one uses functional languages (Wadler, 1998). This paper was central to my understanding of the programming language landscape as I started at Microsoft Research in 1998.  Instead of blaming the unwashed masses for their ignorance, Wadler’s paper outlines seven problems of strongly-typed FP implementations at the time: Libraries, Portability, Availability, Packagability, Tools, Training, Popularity.  It also listed Performance and Ignorance as non-reasons. The early development of F\# was essentially an effort to address each of these.

Further, some responded by trying to compete via new commercial implementations of strongly typed FP languages including Poly ML and Harlequin ML. However, these saw little adoption and left the community with the feeling that the support of a “big player” in the industry was needed. 

A final response was to attempt to use the JVM as a substrate for implementing established functional programming languages, and thereby as a delivery vehicle for FP into the browser and the web (the nascent driving force behind Java at the time).  Foremost in these efforts was MLj, a research/commercial implementation of Standard ML by Benton, Kennedy et al. at Persimmon (REF Benton \& Kennedy, Interlanguage working without tears: blending SML with Java, 1999).  MLj was a whole-program compiler which allowed interop with the Java ecosystem through object programming extensions. When the research arm of Persimmon folded in 1998, Benton moved to MSR Cambridge, followed later by Kennedy, bringing experience highly relevant to .NET and later F\#. Despite these various responses, there was also strong anathema to object-orientation in theoretical communities: proponents of OO were too readily labelled with the tar-brush of heresy: “unprincipled nonsense”, “lacking theoretical foundations” and similar.  

That completes our summary of the general surrounding context as I joined Microsoft Research in 1998 and began precursor work leading to F\#. For completeness, the background influences I am aware of were as follows:

\begin{itemize}
\item I had used strongly typed functional programming, mostly in the context of theorem proving systems (Edinburgh ML in HOL88, Standard ML of New Jersey in HOL90, Caml-Light in HOL-Lite, ForteFL at Intel). I had come to love them, while appreciating their weaknesses. In my undergraduate work I had been supervised by one of the originators of ML, Malcolm Newey. Through my PhD work, the OCaml community and MSR Cambridge, I was involved in overlapping communities that saw strongly typed functional programming as the norm.
\item I had used object-oriented languages (C++, Java) including studying Java and the JVM formally as part of my thesis work.  My experience with C++ at university in 1992 had been negative, particularly through the over-use of hierarchical classification in student projects – both as a modelling technique and its encoding in class hierarchies.  
\item As a child, from 1980-87, I had used BASIC and Logo (Apple II) and Turbo Pascal (Windows). As a student, I used Prolog, C, Scheme, Modula 2. A comparative programming languages course provoked interest in a range of languages. In early employment I had used Prolog on Windows for an Australian software company (SoftLaw, 1990-1993). 
\item I had implemented several strongly-typed language, proof and compilation systems as part of my PhD thesis work using various ML dialects and toolchains including SMLNJ, MoscowML, Caml-light and OCaml. Additionally, I had, somewhat unusually for the times, also implemented some visual tooling for these systems, notably a graphical proof editing IDE for HOL90 (Syme, 1995) and a proof editing workbench for the theorem prover DECLARE (Syme, 1999).  I had a positive disposition to IDE tooling and understood the interaction between IDE tooling and language design.
\item In 1996-98, I had been exposed to the work of academic leaders such as Drossopoulou, Leroy, Wadler and Odersky to synthesize OO and functional programming (Alves-Foss, 1999).
\item I was part of discussions trying to reimagine how we deliver strongly-typed functional programming to “the masses”.
\end{itemize}

\section*{Project 7 and .NET Generics}

When Project 7 kicked off at Microsoft, the researchers at MSR Cambridge recommended the following languages for inclusion on the academic stream: Eiffel, Mercury, Standard ML, OCaml, Scheme, Alice and Haskell.  The biases of the research group at MSR are clear here: 6 of 7 recommendations were strongly typed languages, and 3 of 7 were firmly “strongly typed functional languages” in a specific sense of the term, e.g. incorporating Hindley-Milner type inference and having functions as first-class values. Commercial languages in Project 7 included Perl, Python, Cobol and Ada. Academic or commercial partners were found for each, funding was provided by Microsoft and workshops were arranged at MSR Cambridge and elsewhere.

In retrospect Project 7 was flawed but not catastrophically – some of the researchers didn’t engage, few of the language implementations saw much use, and the costs to maintain them were high. While you can still buy and use Cobol.NET today, .NET programming is dominated by two Microsoft-supported languages C\# and F\#, and the JVM has a more vibrant multi-language ecosystem. However Project 7 did have definite technical impact: for example, at this stage, Gordon and Peyton Jones engaged with the designers of .NET, and argued successfully for the inclusion of tailcalls as a first class operation (the “tail.” instruction in the .NET bytecode), both to support some of these languages and as a way of differentiating the .NET bytecode from the JVM.  This started .NET down a long technical path of innovation and differentiation led by the demands of the languages being brought to the platform.   

Project 7 also had an impact by raising the question of “language interoperability”: it was one thing to get languages targeting a common substrate, another to get them to interoperate.  In 1999, I and colleagues wrote the internal whitepaper “Proposed Extensions to COM+ VOS”  which argued that 

\begin{quote}
a primary objective of the COM+ Runtime is to deliver services and performance that are clearly technically superior to those provided by other potential backend runtime environments. 
\end{quote}
and that Microsoft should “get serious about language innovation”.  Five technical features were proposed, of which “generalized delegates” (i.e. functions as first-class values) and “enhanced parametric polymorphism” were the more serious.  The influence of Pizza and GJ is strong here and these are explicitly mentioned as competitors. I also developed ILX, an extension to the .NET bytecode incorporating these features, which I hoped might be adopted by other Project 7 languages, implemented on .NET initially by erasure and compilation to the existing .NET IL. (REF: Syme, 2001).


This whitepaper served as the start of the “.NET Generics” project, specifically designed to bring a form of generics to .NET that could work for both C\# and other Project 7 languages such as Eiffel, OCaml and Haskell.  .NET Generics and its history is covered elsewhere  and over the next 4 years, Syme, Kennedy and Russo worked with enormous dedication to deliver .NET Generics in C\# and .NET (REF: Kennedy \& Syme, 2001). The feature encountered enthusiasm, reluctance and indifference from various parts of Microsoft, though a review to Gates in 2001 was well received and started to turn things around.   Ultimately the feature was delivered as part of the 2005 .NET 2.0 “Whidbey” release.  At the same time, Microsoft began to make its first very tentative steps towards embracing open source, and a “shared source” release of the .NET codebase was made called Rotor along with a corresponding extension containing the .NET Generics implementation called Gyro.  A poster from MSR’s internal tradeshow “Tech Fest” is shown in Figure 1. 

\begin{figure}

FIGURE: Figure 1 - .NET Generics poster at TechFest 2002, Microsoft Building 33, Redmond
\end{figure}

The key premise of .NET Generics is that generic instantiations can be “managed” by the runtime environment, inclnuding the management of runtime type information and the JIT-compilation of fresh code for newly encountered instantiations.  This avoids the need to either tag or box integers and other “unboxed” values – a technique normally needed when combining polymorphism and separate compilation, because the runtime is able to specialize code on-demand.

This means the end-programming model in, say, C\#, can support a form of generics that is very complete and smooth from the programmer’s perspective: runtime type information is accurate, the process of making and managing instantiations unobtrusive, the code for instantiations is automatically shared based on a policy. .NET Generics has been successful: it is widely adopted by millions of C\# and F\# programmers; it is seen as a key differentiating factor of C\# over Java; and has been the basis for many later innovations delivered in F\#, C\# and .NET. For example, generic collections (C\# 2.0), LINQ (C\# 3.0), tasks (C\# 4.0), async/await (C\# 5.0) and Span (C\# 7.2) all use .NET Generics heavily, as do all F\# features.  .NET Generics put .NET years ahead: even today systems such as Java, Go and Scala struggle with the implementation of aspects of genericity such as supporting instantiation at both reference and value types.  Equally, generics is a technical feature that imposed significant costs on Microsoft’s .NET implementation going forward. Generics is most easily implemented via a JIT and attempts to do fully static compilation of .NET code have struggled with the feature.


From the perspective of the history of F\# (which did not yet exist), the successful delivery of .NET Generics intentionally made .NET a suitable substrate for a “direct” compilation from a strongly typed functional language into .NET bytecode: this was by design, not by accident. For example, it allowed a simple, direct compilation of genericity inferred via Hindley-Milner type inference into .NET Generics with little or no runtime overhead.  Consider simple code such as this in some ML-like dialect:
\begin{verbatim}
let keyAndData getKey x = (getKey x, x)
let data = [| 1 .. 100 |]
let add x = x + 1
let y = Array.map (keyAndData add) data
\end{verbatim}

Here the generic code has been instantiated at integer type. In many systems of generics such as GJ, values of generic type such as parameter \texttt{x} to \texttt{keyAndData} would be represented in boxed (heap-allocated) form.  Thus, in the absence of other optimizations, the code above would cause the boxing of the integers as they enter the (generic) \texttt{keyAndData} function, and then unboxing as they are passed on to the (non-generic) \texttt{add} function.  Such implicit costs for basic collection types would be unbearable and make any Hindley-Milner type-inferred language intrinsically low-performance on .NET. With .NET Generics these specific performance problems go away.  In crucial ways .NET Generics laid a foundation for later work on F\#.


\section*{The Decision to Create F\#}

At MSR, Project 7 also led to the SML.NET project (REF: Benton, Kennedy, \& Russo, 2004).  SML.NET was a continuation of MLj, mentioned earlier, retargeted to .NET.  SML.NET used a sophisticated whole-program optimizer with de-virtualization and representation transformations and was a faithful implementation of Standard ML with extensions for object programming. The system was of high quality but didn’t gain significant external mindshare.   During 2001, I grew frustrated with SML.NET, which was not yet released even though .NET itself was now public. While respecting the research goals of my colleagues, I was keen to see strongly typed FP delivered in a way that could be readily adopted by large numbers of programmers, and on a path to addressing the seven major themes identified by Wadler in 1998.  The implementation of OCaml was influential on me here: OCaml used a relatively direct and simple compilation strategy, and it was not clear that a whole-program compilation strategy was needed to achieve acceptable and reliable performance.  Further, SML.NET didn’t target .NET Generics, and there was no definite plan to make it do so: the compiler was predicated on the benefits of whole-program compilation and pervasive monomorphization, with the aim of recovering performance and compact code.  As commonly happens in research labs, a divergence of opinion occurred.


Initially, in late 2000, in conjunction with Reuben Thomas, I attempted an implementation of Haskell for .NET, using a direct translation from the “Core” intermediate representation of the Glasgow Haskell Compiler (GHC) to the .NET bytecode. This experience was partly successful: small programs ran. However, the advice of Simon Peyton Jones led me to believe that Haskell.NET couldn’t be successful for several technical and cultural reasons: 

\begin{itemize}
\item As with other Project 7 languages, running Haskell on .NET “in isolation” was not enough in itself: a primary goal was to make a functional language that was fully part of the .NET ecosystem, with full interop with .NET libraries. 
\item Full interop means that every .NET function would need a rendering in Haskell with a Haskell type, so type translation is needed. The type systems were not the same, so the translation is onerous or simply impossible in many cases.  
\item Moreover, to ease the translation, Haskell itself would need to be adapted to incorporate some form of subtyping and object programming and would eventually need the ability to extend an existing .NET class.   The Haskell community was reluctant to contemplate such substantial language changes driven by the requirements of a particular platform. 
\item At the time, almost all Haskell code (if you include libraries) needed technical features that lacked corresponding .NET support, including higher-kinded type variables, lightweight concurrency, exceptions (with Haskell’s exception semantics), ephemerons and software transactional memory. So, even interop aside, it would be hard to claim that any Haskell program would run well on .NET; only a subset would do so.   
\end{itemize}
So, work on Haskell.NET stopped.

The question of OCaml and JVM/.NET was also being discussed on the Caml mailing list around this time.  An example is the following message from myself, on February 6, 2001:
\begin{verbquote}
Subject: OCaml on CLR/JVM? (Was RE: OCaml <--> ODBC/SQL Server)

> What I cannot find around is a way to easily interrogate and interface 
> in OCaml with an ODBC data source...

Now I have to say the obvious: wouldn't it be wonderful if Caml interfaced with either Java or the .NET Common Language Runtime seamlessly so we wouldn't have to keep facing these kinds of questions and problems, and could just leverage existing libraries?   

I'm very interested to know if there are people with some time to spare who would be keen to work with me toward a .NET version of OCaml.  I've talked this over from time to time with Xavier, and have done a lot of foundational work for the core language when building a .NET compiler for Haskell.  If you think would be interested, or would simply like to join a mailing list devoted to talking about getting Caml running and interoperating on .NET, then please let me know! 
\end{verbquote}
This was the first explicit public indication of my desire to create a version of OCaml targeting .NET. Leroy replied on February 8, 2001:
\begin{verbquote}
I've been working on and off (mostly off, lately) on an OCaml/Java interface that works by coupling the two systems at the C level via their foreign-function interfaces (Java's JNI and OCaml's C interface).  This was strongly inspired by the work of Erik Meijer et al. on a similar Haskell/Java interface.  (These Haskell guys sure are at the bleeding edge of language interoperability.  This is the second interop idea I steal from them, after the IDL/COM binding.)

The low-level coupling is surprisingly easy, including making the two garbage collectors cooperate: both the JNI and OCaml's C interface provide enough functionality to get the coupling to work without *any* modification on either of the implementations.  How nice! The only limitation is that a cross-heap cycle (a Java object pointing to a Caml block pointing back to the Java object) can never be reclaimed... (Thanks to Martin Odersky for pointing this out.)

Of course, the low-level interface is type-unsafe, so the real fun is to build a type-safe view of Java classes and objects as Caml classes and objects, and conversely.  I'm still struggling with some of the issues involved.  For instance, it turns out to be much simpler (for the implementation, not for the final user!) to map Java objects to values of abstract Caml types, and treat methods as functions over these abstract types, than mapping Java objects to Caml objects.  That was quite unexpected!

One thing I learnt is that the real problem with language interoperability is not how to compile language X to virtual machine Y (this can always be done, albeit more or less efficiently), but rather how to map between X's data structures and objects and those of all other languages Z1 ... Zn that also compile down to Y.  This is obvious in retrospect, but I think many (myself included) often overlook this point and believe that compiling to the same virtual machine is necessary and sufficient for interoperability.  It is actually neither necessary nor sufficient...

While this work started with the JVM, I'm pretty sure it can be made to work with the .NET CLR, as soon as it will have a foreign-function interface with features comparable to those of the JNI.  (And I'm sure this will happen eventually, not only because it makes sense, but also because Java has it, so .NET must too :-)
Stay tuned for further developments. 
\end{verbquote}

This lays out the basic question many languages have faced since: should a language have its own runtime and interoperate indirectly with .NET and/or the JVM, or should it target those runtimes directly?  Leroy’s response represented a divergence of opinion: Project 7 had envisaged very close interoperability, sharing one virtual machine including memory, code, reflection, JIT, GC and library capabilities, and potentially bringing the object system of the host ecosystem into the language.  The approach described by Leroy was, technically, highly sensible for the existing OCaml implementation, however it didn’t feel right once .NET could be assumed. To me, it would intrinsically run into performance, interoperability, tooling and other issues at boundaries between the languages, and adoption would be limited to the intersection of those willing to rely on both the .NET and OCaml implementations.

The discussion also brought contributions from Dave Berry, based on his prior experience of implementing Harlequin’s MLWorks , a proprietary implementation of Standard ML (Dave later contracted with MSR Cambridge on an open source version of .NET Generics), on February 9, 2001:
\begin{verbquote}
> > Now I have to say the obvious: wouldn't it be wonderful if Caml interfaced with either 
> > Java or the .NET Common Language Runtime seamlessly so we wouldn't have to 
> > keep facing these kinds of questions and problems, and could just leverage existing 
> > libraries?   

Although this view is understandable, I think it is rather naive. ... To look at it another way, OCaml already shares a platform with C (at least with the native-code compiler), so all the C libraries are already available... Yet it can still be a lot of effort to link with a C library.  Why should Java and .NET be any easier?  Also, look at the effort that went into making an ML/Java system with MLj... Threads are another area of potential problems.  In fact they can be a total minefield.   
\end{verbquote}
To which I replied on February 10, 2001:
\begin{verbquote}
There's hard work to be done to realise this vision, but in principle a clean interop story sure beats the endless rehashing of other people's code in language X as a library in language Y.  Myself and others involved in the Project 7 are working on one approach to achieve this interop, i.e. compiling languages directly to .NET MS-IL, in the style of MLj, often adding extensions to the language in order to improve the interop.  We are also working on improving the .NET infrastructure, proposing support for features such as parametric polymorphism in MS-IL.  

Xavier is also working on a solution for OCaml, as he mentioned, though the problem of how to reflect the constructs of an object model into ML, Haskell or OCaml remains similar whichever approach you take to actually running the stuff.

There are several reasons why it is easier: exceptions, for example, can be propagated across the interop boundary, without any effort at all if you compile to MS-IL or Java bytecode.  If you're compiling to bytecode you can also ensure more compatibilities of representations, e.g. make sure ML int64's are exactly representationally equivalent to C's int64s.  Note if you don't compile to a bytecode then you even have to marshal integers across the interop boundary in Caml, though this could be automated.

You can also transfer objects more consistently, as the semantics of the object models of Java and .NET are fairly simple in contrast to C, e.g. no need to have an IDL to help interpret pointers as "in-out", "in", "out" parameters.

While at a certain level I like Xavier's approach, i.e. maintaining two runtimes, garbage collectors etc., I have troubles seeing it scaling to the multi-language component programming envisioned as part of .NET approach (and indeed currently in practice with C\#, C++, VB.NET and other .NET languages).  Two GC's are already trouble enough (performance might suck as they will both be tuned to fill up the cache), but if you have components from 10 languages in one process?  10 GCs competing for attention?  Maybe it can be made to work, but there's a certain conceptual clarity in just accepting that a GC should form part of the computing infrastructure, and share that service.  These are the aspects of the .NET approach that I find quite compelling.

As an aside, I think it would be an interesting question to say "OK, let's take it for granted that the end purpose of our language is to produce components whose interface is expressed in terms of the Java or .NET type systems, but which retains as many of the features and conceptual simplicity of OCaml and ML as possible."  I'm not sure exactly what you'd end up with, but whatever it was it could be the language to take over from C\# and/or Java (if that's what you're interested in...)  But without really taking Java/.NET component building seriously right from the start I feel you're always just going to end up with a bit of a hack - an interesting, usable hack perhaps, but not a really \emph{good} language.

Probably the greatest recurring technical problem that I see in this kind of work is that of type inference, and the way both the Java and .NET models rely on both subtyping and overloading to help make APIs palatable.  Type inference just doesn't work well with either subtyping or overloading.  This is a great, great shame, as it's obviously one of the main things ML has to offer to improve productivity.  

P.S. As for threads - I don't think the story is half as bad as you might think.  After all, OCaml threads map down to Windows threads at some point, and I just don't see that there are that many special logical properties of typical ML and Caml threading libraries that make it semantically ridiculous to share threads between languages (though it is true asynchronous exceptions can make things hard when compiling to a bytecode).  But I'll admit I'm not an expert on this. 
\end{verbquote}
Finally, there was techno-political controversy too, this time in a reply from Fabrice le Fessant on February 12, 2001:
\begin{verbquote}
Is the .NET VM open source ? Which part is Microsoft-independent ?...

If Microsoft wants its new product to be used, it is Microsoft problem to port more languages to its VM, and not only say: "We have ported our homemade languages to it (C\#, C++, VB.NET) [because it was designed for them], so, you see, we have proved it's the universal VM. Now, do the same for your languages, or your language will not be used anymore by our customers..."

So, why do we really need a .NET port of OCaml ? OCaml is working fine on Windows, and on many other OS ... 
\end{verbquote}
A discussion thread followed on the merits of open source, standards, interoperability and cross-platform execution, issues which weren’t resolved for F\# for another 13 years, when F\#, C\# and .NET Core were finally open source and cross-platform.  A contribution by Dave Berry on February 16, 2001 was more positive:
\begin{verbquote}
I think Microsoft should be congratulated on their outreach to programming language researchers.  I for one would certainly welcome a widely distributed VM that is a good target for compiling ML.  Interoperability with other languages on the same VM would be a bonus... That said, interoperability is still hard...
\end{verbquote}
There were many valid arguments and sensitivities here, and I proceeded from this point determined to be highly respectful towards OCaml and its existing user base: I genuinely loved the language and the approach to programming it represented. 

Predicting the future trajectory of software infrastructure like .NET and architecture was also an important factor in making decisions, e.g. in this final response by Arturo Borquez on March 3, 2001:
\begin{verbquote}
Perhaps I am wrong, but let me state what I believe about this stuff.... C\# is not really important as it will never reach the 'mass' of VB... The real issue is ... the Client/Server model ... In my opinion this model has no future, ...clients would become minimal.... with a diverse and broad family of client devices (terminals). My conclusion is CLR/JVM ... are not important for the future of Caml, as all will die. Caml will need only some library updates to match the communication tech upgrades.  
\end{verbquote}
In hindsight, predictions like these were both right and wrong: the structure of applications evolved extensively, and .NET and the JVM ultimately de-emphasized their role as “middleware”, but neither .NET nor the JVM have died.  Languages and runtimes seem to endure longer than software architectures.

So, in mid-2001 the itch remained: how was MSR going to bring strongly typed functional programming to .NET in a way that could be readily adopted by large numbers of programmers?  By October 10, 2001 I felt firm enough in this conviction to reply as follows:

\begin{verbquote}
When time permits I plan to implement a .NET CLR compiler for Caml. Initially I will implement only the core language, and perhaps first-order modules, and then to assess things after that.  I will be coding the implementation up from scratch rather than using the sources for the existing OCaml compiler...

My first reason for doing this is because I have an existing OCaml code base that I would like to make available as a .NET library...  Plus I love Caml, and would like to see it supported on .NET, and I'm interested in proving that interoperability between functional languages is practical in .NET. 

This implementation path would give object introspection capabilities for free.  However it would no doubt be slower than the existing native code Caml implementation: you don't get something for nothing.

I don't know of any other \emph{active} efforts to do a .NET compiler for Caml.  SML.NET will, hopefully, be available publicly soon.
\end{verbquote}

So by late 2001 this path remained: to bring a variant of the OCaml language to target .NET itself. The Project 7 effort around OCaml had led to the above approach by Leroy and didn’t look likely to continue.  This left a space for a new Caml.NET initiative, though one targeting the .NET IL itself, and in December 2001 I decided to move ahead with an “Caml.NET”. This was later rebranded “F\#” after private discussion with Cedric Fournet and Georges Gonthier, to allow for greater divergence from OCaml and to bring language experimentation into scope.   


\section*{Early F\# - 2002 – 2003}

The early conception of F\# was simple: to bring the benefits of OCaml to .NET and .NET to OCaml: a marriage between strongly typed functional programming and .NET.  Here “OCaml” meant both the core of the language itself, and the pragmatic approach to strongly-typed functional programming it represented. The initial task was relatively well-defined: I would re-implement the core of the OCaml language and a portion of its base library to target the .NET Common Language Runtime. The implementation would be fresh, i.e. not using any of the OCaml codebase, for legal clarity. 

The first lines of the F\# implementation were written in December 2001, a front-end for a re-implementation of the core Caml syntax targeting ILX as a back end, and thus to .NET. The initial compiler was written using OCaml (later bootstrapped using F\# in 2006). 

The initial design choices were subtle.  By far the most wide-ranging design decision is easy to miss in retrospect: after choosing OCaml as a starting point, the most significant design choice made for F\# was that it be a .NET language.  Everything else was to be subservient to that goal.  In particular, .NET types are F\# types, .NET values are F\# values, .NET exceptions (and their semantics) are F\# exceptions (and their semantics), and .NET threads are F\# threads.  The same was true in reverse and “two-way interop” was always a design goal.  There’s no type translation, no marshalling from one representation to another. Strings in F\# were to be strings in .NET and vice-versa.  Types and functions defined in F\# could be used from other .NET languages. This decision gave F\# less room to innovate – more often than not, F\# is stuck with whatever .NET does – but it guaranteed two-way interop.  This was a huge reason for starting a new language design, rather than trying to map an existing language onto .NET.  This full identification of types and data goes beyond the question of having one runtime vs two: even if you have one runtime, a language could still have chosen to use different representations for (say) a list of integers, represented internally as .NET objects of some kind, but marshalled when passed to a .NET method: one runtime, but two representations. F\# doesn’t do that: it uses one runtime and, where possible, identical representations. This influenced many small decisions: for example, from the outset a function declared in F\# had a guaranteed, stable representation in .NET code as a static member of a class with a stable name, and could be used directly from .NET languages.  This also meant F\# code could always be accessed via .NET reflection.  Although the first version of F\# was initially presented as “Caml-for-.NET”, in reality it was always a new language, designed for .NET from day 1. F\# was never fully compatible with any version of OCaml, though it shared a compatible subset, and it took Caml-Light and OCaml as its principal sources of design guidance and inspiration.  

In addition, there was the question what not to implement.  A notable omission from the design was the functorial module system of OCaml.  Functors were a key part of Standard ML and a modified form of the feature was included with OCaml, a source of ongoing controversy amongst theoreticians.  I was positively disposed towards functors as a “gold standard” in what parameterization could be in a programming language, but was wary of their theoretical complexities. Furthermore, at the time there were relatively few places where functors were used by practicing OCaml programmers. One part of the OCaml module system – nested module definitions – was eventually included in the design of F\#.  However, functors were perceived to be awkward to implement in a direct way on .NET and it was hard to justify their inclusion in a language design alongside .NET object programming. Another decision was not to include any OCaml 3.0 features, specifically neither the object system nor the recently added “named arguments” feature.  Leroy’s email above explains the issues regarding the object system: there was sufficient disparity and mismatch between the object systems of .NET and OCaml that the latter couldn’t be used for the former.  The OCaml pre-processor CamlP4 was also not supported, though CamlLex and CamlYacc could be used. The question of the object system would be dealt with later.  However, this meant that F\# and OCaml diverged as of the core language of OCaml 2.0.


The first release (v0.1, soon replaced by 0.5) was made near-silently on June 4 2002  as an addition to the ILX project, making the following claims on the website:
\begin{verbquote}
Mixed functional/imperative programming is a fantastic paradigm for many programming tasks....You can access hundreds of .NET libraries using F\#...F\# is an implementation of the core of the Caml programming language for the .NET Framework, along with cross-language extensions. ...The aim is to have it work together seamlessly with C\#, Visual Basic, SML.NET and other .NET programming languages.  ...Types and values in an ML program can be accessed from some significant languages (e.g. C\#) in a predictable and friendly way. ... F\# provides an implementation of a subset of the OCaml libraries as well as the ability to access .NET libraries.  Using the .NET libraries is optional.... F\# supports features that are often missing from ML implementations such as Unicode strings and dynamic linking.  ... Tooling consists of a simple command line compiler, supporting separate compilation, debug information and optimization... F\# is, as far as I know, the first ML compiler to have good binary-compatibility and versioning properties....
\end{verbquote}

Some hurdles had been cleared along the way. MSR granted permission to allow commercial use of programs compiled with ILX and this permission was recycled for the F\# implementation. Next, at a conference I asked Leroy for tacit approval in putting out a variant of Caml for .NET, including making changes to the language design.  Leroy approved – OCaml itself was part of a long history of adapting and modifying the core ML – and what was research if we didn’t experiment?  In a later email reply Leroy said:

\begin{verbquote}
Don Syme and his Microsoft Cambridge colleagues did a great job with adding parametric polymorphism to the .NET framework -- something that was initially overlooked in .NET --, and I'm very happy that they chose core Caml to demonstrate this extension in action. https://caml.inria.fr/pub/ml-archives/caml-list/2002/06/8d07fd5058aa26127d1b7e7892698386.en.html 
\end{verbquote}

To which I replied:
\begin{verbquote}
And I'm even more grateful to Xavier and the team for doing such a great job with OCaml over the years, and for providing a solid core language, an excellent runtime system and the very interesting set of language features they've added to the core.  Core Caml provides a great starting point for work of all kinds: I used it in my PhD thesis, for example, as the term language for a theorem prover.

I chose to implement a core Caml compiler for .NET partly to test out generics, but also because I want to be able to program against .NET libraries using the language I love to program in, and reuse the libraries and techniques I've developed.  I guess it's possible I'll get a bit of flak from the Caml community about F\#.  Being at Microsoft Research I presume I'll be writing a fair bit of .NET code sooner or late, and personally I'd rather do that in Caml/F\# than C\#... I hope the Caml community won't mind me making that opportunity available to others via the public release of F\#.  
\end{verbquote}
The first real design-work began with the addition of the ability to access .NET object types via the dot-notation.:
\begin{verbquote}
C\# and other .NET languages can be directly accessed from F\#...  Types are accessed using the "Namespace.Type" notation.  You may simply use "Type" if an "open Namespace" declaration has been given. Instance members are accessed using "obj.Method(arg1,...,argN)" or "obj.Property" or "obj.Field". Static members are accessed using "Namespace.Type.Method(arg1,...,argN)" or "Type.Method(arg1,...,argN)", similarly for properties and fields. 
\end{verbquote}

While seemingly innocuous, this design decision broke with OCaml and a long tradition of ML language design: it used inferred type information in name resolution. A name like M in obj.M was now resolved immediately using the partially inferred type of obj rather than by adding a new inference constraint. This meant that type annotations would now sometimes be needed, compromising one of the traditional “rules” of ML (i.e. that type annotations are strictly optional), and that inference becomes “algorithmic” or “left-to-right”:

\begin{verbquote}
Typing. Sometimes extra annotations are needed to get the program to typecheck, e.g. casts using "(cast <expr> : <type>)" and type annotations to help resolve overloading.
\end{verbquote}

I decided that if the inference algorithm was well-defined and kept stable, this would be sufficient for interoperability purposes. In practice, the use of partially inferred type information in name resolution proved effective and stable and was kept throughout the evolution of F\#.  Type inference was eventually specified algorithmically in the language specification. 

Another design question was about nulls. The question was not one of safety: like the JVM, the .NET runtime would itself perform null checks when values were accessed. Instead, it was a matter of program correctness. The SML.NET system had “sanitized” all interop calls by inserting the Standard ML “option” type with tags SOME/NONE at all relevant points.  In F\#, I decided not to do this:
\begin{verbquote}
Null.  Null objects returned by the .NET assemblies are NOT checked by the process of importing the assemblies or by the F\# type system.  This may be addressed in the future, but for the moment use the "nonnull" function from Pervasives to check if values are null and the "null" value from Obj to create a new null value. https://web.archive.org/web/20020814185220/http://research.microsoft.com:80/projects/ilx/fsharp-manual-import-interop.htm
\end{verbquote}
Instead, the rule adopted was that .NET-declared types would allow the use of “null”, while F\#-declared nominal types would not.  This kept a strict approach to nullness within F\#-only code, in keeping with OCaml but allowed the use of null in interop scenarios with .NET types. This was partly because of ergonomics: the insertion of the option type was highly intrusive on programming and nulls were not used as pervasively in the .NET libraries as in Java, so in balance the need for a pleasant programming experience outweighed the need for null-safety at interoperability. Further, I felt that the topic of null-safety should be dealt with systematically across all .NET languages, as we had done with .NET Generics.   

Initially, early F\# avoided adding object programming declarations:
\begin{verbquote}
Currently you cannot declare new classes or implement interfaces in F\#.  For the moment workaround this by declaring a new class in C\# that accepts delegate parameters to implement the virtual/interface members, and then pass function values from F\# to the C\# class.  You will only need to write this C\# class once.  \end{verbquote}

Further, contrary to the warnings from Dave Berry and others in the email threads shown earlier, no design work was needed for threading: F\# simply assumed the same threading model as .NET itself, which essentially mapped .NET threads to operating system threads.



\section*{Early F\# - Release}


F\# “0.5” was little noticed at first, deliberately: the initial implementation was lacking in many ways and needed time to settle. Initially, the plan was as follows: 
\begin{enumerate}
\item Make the language viable for adoption and use.
\item Use it to stress-test .NET Generics.
\item Get it out to the public.
\item See where things led.
\end{enumerate}

I had been influenced by my time at Intel Strategic CAD Laboratories, which used a structured “maturity model” for research projects and technology development: projects at Intel would proceed from “concept” to “proof of concept” to “prototype” and then through a product delivery phase. Thus there was no Microsoft “buy-in” at this stage: few at the company knew of F\# apart from those in MSR Cambridge and their .NET team contacts. 6 months later, after several iterations, the project got noticed by Internet news sites, always keen for the latest scoop, and I decided to make a clarification on the OCaml mailing list in case things “got out of hand” before the implementation was fully ready, and in case accusations of “embrace, extend, extinguish” emerged. (Boulton, 2003)

\begin{verbquote}
There have been some utterly speculative (and entirely off-the-mark!!) internet press reports about this project in the last few days (e.g. see internetnews.com).... I thought it wise to add the following clarification to the F\# website and to post it to this list.

...Despite reports suggesting otherwise, F\# is a relatively small research project designed to demonstrate that it is possible to easily implement ML-like languages for use on the .NET Framework.  There are no current plans to commercialize F\#.... F\# is public, on-going research, and Microsoft Research regularly and openly collaborates with universities on programming languages. (Syme, 2003)  
\end{verbquote}

The fact that F\# needed to be down-played initially was partly due to the sensitivities around launching anything “product-like” at the time from MSR. At the time, all public software by MSR had an awkward legal/commercial status: publication of software was primarily to support a research/publication agenda. Despite a budget nearing \$1B, the organization was not at that stage permitted to make and release commercial products.  MSR strongly encouraged open research, but open software was more problematic. However, designing and delivering new programming languages was an essential part of any PL research agenda, and indeed the whole rationale behind Project 7.  Further, external “proofing” of these technologies was critical to refine them. 

External perceptions were also tricky to manage: from the perspective of computer science academia and hacker culture, corporations in general – and Microsoft in particular – were often seen as structural adversaries. Offerings from MSR were even feared, and one leading researcher suggested that F\# would “kill off” language research.  In retrospect such ideas seem laughable – PL research has bloomed in the last 15 years and hundreds of new languages have been developed – but these views stemmed from anti-commercial biases, fear of a perceived monopolist, and Microsoft’s opposition to open source software at the time. 

Either way, my belief was that, in the area of programming languages, you had to go public and be commercially usable in order to be influence programming practice, and to be true to both the spirit of research and the original goals of Project 7. Later, other cutting-edge MSR projects would not reach their full potential, because they didn’t make the commercially usable releases necessary to proof the technologies and gain evidence of their utility in sufficient time to occupy a market niche, examples include Accelerator and Dryad LINQ. On the other hand, MSR provided a good “institutional home” for a language, given its concentration of expertise and its long-term mission to change computing. Lab directors such as Andrew Herbert, Luca Cardelli and Andrew Blake would be consistently supportive of the work on F\# over a long period of time. However, doing a public, commercially usable language offering via MSR was not going to be plain sailing, and the support of the product teams would ultimately be needed.

\section*{F\# 1.0 – 2004-2006 - Overview}

After completing .NET Generics in mid-2004, the rest of the year saw intense work on improving F\#. At this stage, .NET was on the ascendency inside Microsoft and it achieved widespread external success on the back of a huge evangelization effort: most programming for the Windows platform moved over to C\# and .NET worldwide. A massive shift towards .NET also happened internally: the Windows team started major initiatives, including a rewrite of the Windows “shell” and the creation of many major .NET projects such as Windows Presentation Foundation, Windows Communication Foundation and Windows Workflow Foundation.

On January 5, 2005, a pre-release of F\# 1.0 was declared in my first MSDN (Microsoft Developer Network) blog entry.  In March 2005, F\# 1.0 was first demonstrated at “TechFest”, an internal MSR trade-show in Redmond.  

\begin{figure}
Figure 2- Two posters for F\# 1.0 at MSR TechFest 2005
TBD
\end{figure}


F\# developed in crucial ways during 2004-06.  Based on successful trials, and with the support of Byron Cook, MSR manager Luca Cardelli agreed to add developer support to the project. On February 10, 2005 we were able to advertise and on 24 March 2005, James Margetson joined to form a small team with interns (Dominic Cooney, May-July 2004, Gregory Neverov June-August 2006). Small internal and external user communities grew and trust in the project began to form. The technical additions made to F\# during this time were as follows:

\begin{enumerate}
\item Completion of the core Caml-like language programming model (2004)
\item Targeting .NET generics (2004)
\item Addition of initialization graphs (2004)
\item Addition of method overload resolution and object-expressions for interoperability with .NET (2004)
\item Addition of “statically resolved type parameters” for handling overloaded arithmetic in a way that fits with Hindley-Milner type inference (2005)
\item Addition of class/interface constructs for object programming (2005)
\item Addition of implicit class construction (2006)
\item Addition of the “light” indentation-aware syntax (2006)
\item Addition of a treatment of subtyping within Hindley-Milner type inference (2006)
\item Addition of runtime meta-programming via quotations (2006)
\item Addition of F\# Interactive, a REPL for F\# (2006)
\item Initial Visual Studio tooling (2006)
\item Bootstrapping (2006)
\item Execution on Linux using Mono (2006)
\end{enumerate}

To “proof” the language we turned to some existing OCaml codebases at MSR including the SPiM (Stochastic Pi Machine), Static Driver Verifier and Terminator projects.  These tests were successful, for example allowing the addition of a Windows-based GUI to SPiM.   During this time, James Margetson was responsible for performance testing and supporting the internal use of F\# on these projects by Andrew Phillips, Jakob Lichtenberg and Byron Cook. Margetson also implemented the first REPL for F\# and created numerous compelling demonstrations of interactive development using F\# scripting and the REPL. The author and Margetson were responsible for documentation and releases.

During this time F\# was not the result of a “meeting of minds” amongst MSR Cambridge language researchers, but rather myself and collaborators pursuing a series of design additions to the initial implementation, with the help of some feedback from colleagues, users, researcher networks such as WG2.8 and an emerging worldwide community.  The design conversations in the external community on mailing lists and in blog responses were encouraging, and internal and external adoption was growing steadily. 
\subsection*{F\# 1.0 – Pipelines}

One of the first things to become associated with F\# was also one of the simplest: the “pipe-forward” operator, added to the F\# standard library in 2003:
\begin{verbatim}
let (|>) x f = f x
\end{verbatim}
In conjunction with curried function application this allows an intermediate result to be passed through a chain of functions, e.g.
\begin{verbatim}
[ 1 .. 10 ] 
    |> List.map (fun x -> x *x) 
    |> List.filter (fun x -> x % 2 = 0)
\end{verbatim}
instead of 
\begin{verbatim}
List.filter (fun x -> x % 2 = 0) 
   (List.map (fun x -> x *x) [ 1 .. 10 ])
\end{verbatim}
Despite being heavily associated with F\#, the use of the pipeline symbol in ML dialects actually originates from Tobias Nipkow, in May 1994 (with obvious semiotic inspiration from UNIX pipes) :
\begin{verbquote}
... I promised to dig into my old mail folders to uncover the true story behind |> in Isabelle/ML, which also turned out popular in F\#... (e.g. see http://paste.pocoo.org/show/134013/ for some example from Scala -- it cites "F\#'s pipeline operator"). 
In the attachment you find the original mail thread of the three of us [ Larry Paulson; Tobias Nipkow; Marius Wenzel], coming up with this now indispensable piece of ML art in April/May 1994. The mail exchange starts as a response of Larry to my changes.  
...Tobias ...came up with the actual name |> in the end...
\end{verbquote}
The use of the pipeline symbol is particularly important in F\# because type-inference is propagated left-to-right and name resolution occurs based on information available earlier in the program.  For example, the following passes type checking without an explicit type annotation:
\begin{verbquote}
let data = [ "one"; "two"; "three" ] 

data |> List.map (fun s -> s.Length)
\end{verbquote}
In contrast the following requires an explicit type annotation:
\begin{verbquote}
let data = [ "one"; "two"; "three" ] 

List.map (fun (s: string) -> s.Length) data
\end{verbquote}
The F\# library also defined two and three-argument pipeline operators, e.g.
\begin{verbquote}
let (||>) (x1, x2) f = f x1 x2

(0, data) 
    ||> List.fold (fun count s -> count + s.Length)
\end{verbquote}

\subsection*{F\# 1.0 –Tackling Object Programming}

From the outset, F\# consumed class and interface definitions from .NET. Being a functional language, it was natural to begin by supporting an expression-based form of object implementations akin to function closures.  F\# 1.0 described these as follows: 
\begin{verbquote}
An object expression declares an implementation and/or extension of a class or interface. For example, the following specifies an object that implements the .NET IComparer interface:
    { new IComparer with Compare(a,b) = compare a b }
\end{verbquote}

After attempts to allow .NET classes to be declared using OCaml-like record types, on April 27, 2005 I began the process of designing the object-programming extensions for F\#, through an email to Dominic Cooney (no longer an intern, but experienced in using F\# and a sounding board for private discussions):

\begin{verbquote}
We're continually coming across the need to be able to present F\# APIs in a more OO way.  ...I'm wondering if I could run some drafts of both the language mechanisms and the API itself by you for your comments, since you are so familiar with both the library and the standards expected of .NET libraries. 
\end{verbquote}

In the next iteration of the discussion on May 19, 2005 the F\# object programming syntax took its near-final form (with the exclusion of implicit constructors, added later):
\begin{verbatim}
type X =
  override x.ToString() = "abc"

  member x.InstanceProperty = "fooproperty"

  member x.MutableInstanceProperty
    with get() = "fooproperty"
    and set(v) = System.Console.WriteLine("mutated!")
    
  member x.InstanceIndexer
     with get(v) = v+1
     
  member x.InstanceMethod(s1) = "baz"

  static member StaticProperty = "fooproperty"

  static member MutableStaticProperty
    with get() = "fooproperty"
    and set(v) = System.Console.WriteLine("mutated!")
    
  static member StaticMethod(s1,s2) = "static method"
\end{verbatim}

In this syntax, "x" is the name of the “this” or “self” parameter and its use in declarations such as "member x.InstanceProperty" represent binding occurrences.  The decision to use a user-defined explicit name for this parameter was partly driven by similar decisions in the OCaml system, and partly by the feeling of horror I had experienced while refereeing an academic paper on the subtleties of the resolution of “this” in Java inner classes.  Since nesting of such constructs would eventually be required, and considered normal in an ML-family language, it would be better to require an explicit name.

In retrospect, the addition of object programming to F\# was a process of “deconstruction” of object-orientation into its essential elements of roughly 20 individual features: dot-notation, classes, method-overloading and so on.  I later formalized this list in a private email as follows.

Acceptable features related to object programming quickly incorporated into F\#:
\begin{enumerate}
\item Instance properties and methods and type-directed name resolution
\item Implicit constructors 
\item Static members, i.e. using type names as qualifiers
\item Indexer notation arr.[x]  
\item Named and optional arguments on methods
\item Non-hierarchical interface types
\item Object expressions 
\item Explicit interface implementation on object, record and union types)
\end{enumerate}
Object programming features that are “ok when necessary” but need some justification and thought: 
\begin{enumerate}
  \setcounter{enumi}{9}
\item Mutable data (if you're using imperative state)
\item Defining events (an imperative .NET standard but not such a bad one)
\item Defining operators on types (rarely needed but useful for interop)
\item Auto properties (I don't use them much but they are useful for interop)
\item Implementing IDisposable and IEnumerable (both imperative, but pretty normal and a reasonable standard)
\item Tasteful uses of type extensions
\item Structs (for performance)
\item Delegates (for interop)
\item Enums (for interop)
\end{enumerate}
Object programming features that are included in the design of F\# but whose use is generally “going too far”, unless there are explicit performance goals, or API-usability goals, or binary compat goals  achieved:
\begin{enumerate}
  \setcounter{enumi}{18}
\item Implementation inheritance
\item Nulls and \texttt{Unchecked.defaultof<\_>}
\item Method overloading
\item Additional primary constructors (a form of overloading)
\end{enumerate}
Object programming features that are not supported at all
\begin{enumerate}
  \setcounter{enumi}{22}
\item Protected members (they only encourage implementation inheritance).
\item Anything to do with aspect oriented programming
\end{enumerate}

Through this process features were progressively incorporated into F\# in a way that preserved the essence of the core expression language and emphasized delegation over inheritance.  I summarize this today by stating that “F\# embraces ‘object’ programming and de-emphasizes ‘object-oriented’ programming, especially implementation inheritance”.  For example, the “protected” accessibility modifier is not supported even to this day in F\#, since it is perceived to encourage implementation inheritance.

F\# was by no means the only language deconstructing object-oriented programming around this time. Some of this has been discussed in the prior section on the reactions to object-orientation by the academic programming language design community, including the GJ and Pizza languages. Other examples include Effective Java (Bloch, 2001), emphasizing composition over inheritance.


\subsection*{F\# 1.0 – Improving the Functional Core: Initialization Graphs}

The first novel feature added to F\# was an adjustment to initialization and recursion of a kind not previously used in strongly typed functional languages. At the time OCaml already supported recursive definitions of functions (as with all functional languages), and definitions of recursively-referential data such as the following:
\begin{verbatim}
   let xs = 1 :: xs
\end{verbatim}

Here “::” is the OCaml “cons” operator, and the declaration gives an infinite list of ‘1’ values. This is implemented by creating a single allocate cons-cell and “fixing up” the tail pointer to point back to the value itself.  However this technique only applies when allocating values.  It doesn’t support functional abstraction, and doesn’t apply when any computation is involved in value construction.

The F\# approach to this problem area was designed and implemented initially in mid-2004 and presented internally at MSR Cambridge on September 4, 2004 then at the ML Workshop 2005 (REF Syme, 2006). This feature was inspired by the above OCaml feature, and hallway conversations with Georges Gonthier and the idea of giving “co-inductive” interpretations to recursive definitions wherever possible.  Co-inductive techniques – including co-inductive algebras as an interpretation of object-orientation – was a popular research topic at the time. In the 2004 presentation, I focused on how to define networks of “reactive objects”: 

\begin{verbquote}
Forget subtyping.  Forget inheritance.   The restrictions on self-referential and mutually-referential objects is what makes ML a poor GUI programming language....At least when driving reasonable libraries such as System.Windows.Forms, and the problem gets worse the more "declarative" a library gets.... C\# "solves" this through a mishmash of implicit nulls and/or "create-and-configure" APIs.   ML "solves" it in a similar way.  F\# permits the above techniques, but offers another solution... (REF Syme, 2004)
\end{verbquote}

The solution offered was to extend the “let rec” construct to allow the definition of not just functions and directly-allocated recursively-referential data, but also a graph of values and objects produced via computation, e.g.


\begin{verbquote}
F\# permits you to write values (and not just functions) whose specifications appear to refer to themselves, but where the recursive references are hidden inside delayed values such as inner functions, other recursive functions, anonymous 'fun' lambdas, lazy computations, and the 'methods' of object-implementation expressions. 

The recursion is 'runtime checked' because there is a possibility that the computations involved in evaluating the bindings may actually take the delayed computations and execute them. The F\# compiler inserts delays and thunks so that if runtime recursive reference does occur then an exception will be raised.

The recursion is 'reactive' because it only really makes sense to use this when defining automaton such as forms, controls and services that respond to various inputs and make self-referential modifications as a result. A simple example is the following menu item, which prints out part of its state as part of its action:

   let rec menuItem = 
      new MenuItem("Say Hello", 
                   EventHandler(fun e -> printf "Hello %s\n" menuItem.Text), 
                   Shortcut.CtrlH)

A compiler warning is given because in theory the "new MenuItem" constructor could evaluate the callback as part of the construction process, in which case a self-reference would have occurred - and F\# can't prove this won't happen. 
\end{verbquote}

The ML Workshop paper describes the historical precursors to this feature and its technical basis – “recursive use exceptions” can be produced during initialization but once initialization succeeds no further exceptions are possible. The feature is still used occasionally in F\# today, and it later influenced aspects of the design of F\# object programming: in F\# 2.0 class definitions, virtual calls that “recursively” invoke object members in sub-classes during object construction are checked for initialization safety and will raise an exception if reentrancy occurs before initialization is complete. 


\subsection*{F\# 1.0 – Improving the Functional Core: Overloaded Arithmetic }

Among the obvious problems of OCaml was the question of overloaded arithmetic.  OCaml had avoided adding type classes in the style of Haskell and had instead adopted a syntax-disambiguated approach to integer and floating-point arithmetic, e.g.

\begin{verbatim}
    let x1 = 1 + 1      (* an integer*)
    let x2 = 1.0 +. 2.0 (* a floating point number, note the +. instead of + *)
\end{verbatim}

This approach was practical for symbolic programming, which did not use numeric types extensively, but impractical in the context of .NET, which had its own standards in this area. For example, a type supporting overloaded arithmetic would indicate this by supporting a static member call \texttt{op\_Addition}. The F\# approach to solving this problem was inspired by work on HM(X) and G'Caml, a proposal for treating these issues in OCaml (Furuse, 2002).  Specifically, “method constraints” were added, introduced by a deliberately baroque syntax:

\begin{verbatim}
    let inline (+) (x: ^T) (y: ^U) : ^V = 
         ((^T or ^U): (static member op_Addition : ^T * ^U -> ^V) (x, y))
\end{verbatim}

This definition says that any use of “+” is implemented via inlining a call to an appropriately-typed \texttt{op\_Addition} method, defined on the type \texttt{\^T} or \texttt{\^U}, i.e. the type of either the left-hand or right-hand argument.  The \texttt{\^T} notation for type variables indicates statically resolved type parameters (SRTP), i.e. type parameters which are resolved to a nominal type at compile-time.  The inline keyword was added to F\# only to support this construct: by inlining, the constraint would be resolved according to the types available at point of use. This allows overloaded arithmetic to integrate neatly with Hindley-Milner type inference, and code to take a more natural form:

\begin{verbatim}
    let x1 = 1 + 1     // an integer
    let x2 = 1.0 + 2.0 // a floating point number
    let x3 = DateTime.Now + TimeSpan.Years(1.0) // a date
\end{verbatim}

SRTPs subsequently got used more generally in F\# as a mechanism for constrained generics, though originally it was only specifically designed to cope with overloaded arithmetic.


\subsection*{F\# 1.0 – Improving the Functional Core: Active Patterns}

Since the 1980s, one of the best-loved features of strongly-typed functional programming languages has been pattern matching, represented in F\# and OCaml by the match ... with ... construct. Since Wadler’s work on views (Wadler, 1987) it had been recognized that pattern matching suffered a lack of abstraction: you couldn’t write new pattern matching constructs for existing or abstract data types.  During the “proofing” of F\# in 2005 the importance of this problem in real-world OCaml codebases like SPiM and Static Driver Verifier became obvious to me: there were many cases in those codebases where implementation details of types were “leaking out” into code through pattern matching, making changes of core representations difficult.  In early 2006 I began the process of deciding what to do about this for F\#.  

The idea of “active patterns” or “views” had featured in academia but had never been implemented in a practical strongly-typed FP system (Erwig, 1996).  Since F\# had to interoperate with .NET object types whose representations were private, it became natural to add extensible pattern matching. In May 2006, Gregory Neverov joined as an intern and was assigned this topic.  A prototype emerged quickly, and was presented at WG 2.8, July 16-21, Boston.  Simon Peyton Jones gave very helpful advice for F\# at this time, recounting the various attempts to add view patterns to Haskell, an emphasizing the need for “bang for buck” in such a feature, i.e. simplicity of declaration and use. An initial implementation of F\# active patterns was released in August 2006, an ICFP paper followed (Syme, Neverov \& Margetson, 2007), and the feature remains a very widely used part of the F\# language. 

F\# active patterns allow for partial , complete and multi-case patterns. Here is an example of defining partial active patterns to parse a string into an integer or boolean:

\begin{verbatim}
// create an active pattern
let (|Int|_|) str =
   match System.Int32.TryParse(str) with
   | (true, i) -> Some i
   | _ -> None

// create an active pattern
let (|Bool|_|) str =
   match System.Boolean.TryParse(str) with
   | (true, b) -> Some b
   | _ -> None
\end{verbatim}

Once these patterns have been set up, they can be used as part of a normal “match..with” expression.

\begin{verbatim}
// create a function to call the patterns
let testParse str = 
    match str with
    | Int i -> printfn "The value is an int '%i'" i
    | Bool b -> printfn "The value is a bool '%b'" b
    | _ -> printfn "The value '%s' is something else" str

// test
testParse "12"
testParse "true"
testParse "abc"
\end{verbatim}

The design quickly had influence beyond F\#. One attendee of the WG2.8 workshop mentioned above was Martin Odersky, and on July 25, 2006 he replied:

\begin{verbquote}
I enjoyed a lot discussing with you at the WG 2.8. I have been thinking how to do active patterns in Scala. It seems I can replace existentials by dependent types. It is less clear to me at present is how to do GADT like behaviour. 
\end{verbquote}


From this email and the first EPFL paper (Emir, Odersky \& Williams, 2007), it seems that the addition of active patterns to F\# had some impact on the design of Scala. The final versions of the respective mechanisms for F\# (active patterns) and Scala (extractors) were designed and implemented around the same time.    


\subsection*{F\# 1.0 – Improving the Functional Core: First-class Events}

Early F\# applications included GUI programming for systems like SPiM, and inevitably reactive, asynchronous and event-based programming received greater emphasis in F\# than in previous ML-family language designs. .NET metadata and C\# included a built-in notion of “event”. However, this concept sits uneasily in a typed functional language design, for two reasons:

\begin{enumerate}
\item C\# events are “built-in” to the language and .NET metadata, when the intuition is that they belong in a library
\item C\# events can’t be treated as first-class values.  
\end{enumerate}

In the process of designing the F\# object system – and in order to simplify and regularize it – “first-class events” were designed as an F\# language and library extension and released on March 23, 2006 .  The aim was to make .NET events look and feel as if they were just library-defined values, and further allow the combination of events as first-class event values. Aepresentative code sample for first-class events was:

\begin{verbatim}
let mouseMove = 
  form.MouseMove 
  |> Event.filter (fun e -> e.Button = MouseButtons.Left)
  |> Event.filter (fun _ -> inputMenuItem.Checked)
\end{verbatim}

Here \texttt{form.MouseMove} is a first-class event allowing registration and de-registration of handlers.  The event composition filters triggers of \texttt{form.MouseMove} to generate a new event that only fires when the Left mouse button is down and a particular menu item is checked.  Common patterns of event composition, filtering, combination and transformation can now begin to be abstracted.  The addition of this feature directly influenced Wes Dyer, leading to the creation of the Reactive Extensions (Rx) project with Erik Meijer.  Registering event handlers has implications for memory leaks, later dealt with by converting this part of the F\# programming model to use the IObservable type from Rx.  This topic also led to an F\#-related publication by Petricek and Syme on garbage collection in reactive systems (REF: Petricek \& Syme, 2010).



\subsection*{F\# 1.0 – Improving the Functional Core: Computation Expressions and Async}

In April 2007, I spent a six-week sabbatical at EPFL with Martin Odersky in Lausanne.  Odersky was then developing Scala, an exciting time for that language and group. Partly through this visit another important idea was seeded and eventually added to the core F\# design: computation expressions and their application to asynchronous programming.  This would still be influencing C\# years later, in C\# 5.0 (async/await) and 8.0 (async sequences), and in turn influence many other languages. 

The problems addressed by computation expressions and “async” were as follows.  First, from 2003 there was an increasing focus on multi-core and parallel processing in commodity computing systems. Further, the rise of web-programming put server-side concurrency and client-side long-running web-requests to the fore. Additionally, in the context of Windows, it could be assumed that “operating system threads were expensive”, thus ruling out approaches to concurrency based purely on OS threading.  This combination of factors gradually led languages and frameworks to take a strong point of view on concurrency and user-level threading.  For .NET the focus of C\# work at the time was on shared-memory primitives and locking (Duffy \& Sutter, 2008). While good for low-level high-performance primitives, the .NET community were crying out for better, more productive abstractions.  For many other languages, the focus was on actor-like message queueing systems, futures or continuations.  

From the theoretical side, it had long been recognized:

\begin{enumerate}
\item async was a form of monadic programming implemented via continuation passing;
\item adding “syntactic sugar” for monadic computations would make an expressive addition to a programming or specification language.
\end{enumerate}


At EPFL in 2007, I noticed that Philipp Haller had added a react { ... } construct for message processing in Scala (Haller \& Odersky, 2009), and realized that some kind of primitive construct to deal with asynchronous programming was going to be needed in F\#.  C\# had added “iterators” in C\# 2.0 (again initiated by MSR) and this also featured an implicit inversion of control-flow which was of interest in the context of async programming.

Ideas around async programming had also been floating around the OCaml community and its mailing lists, especially through the async implementation used in the system MLDonkey by Fabrice le Fessant, begun in 2001.  Haskell had added monadic syntax , and later arrow syntax.  Many theorem proving systems had generic notational extensions. But no strict functional language had a suitable monadic syntax allowing the re-interpretation of all control constructs in asynchronous form, and in OCaml and Standard ML libraries of functional combinators were generally used instead.  

On returning from EPFL, I discussed these problems with Margetson around May 2007, who emphasized the importance of monads in implementing async programming. This led me to finally experiment with adding a monadic syntax to F\# and apply it to asynchronous programming, leading to the addition of “computation expressions” and async { ... } to F\# in 2007 (Syme, Petricek \& Lomov, 2011).  On October 10, 2007 we announced these features in the blog post Introducing F\# Asynchronous Workflows,  Representative async code samples used in the blog posts at the time were as follows:

\begin{verbatim}
    let task1 = async { return 10+10 }
    let task2 = async { return 20+20 }
    Async.Run (Async.Parallel [ task1; task2 ])
\end{verbatim}

Here:
\begin{itemize}
\item \texttt{async \{ return 10+10 \}} generates an object of type \texttt{Async<int>}.  
\item \texttt{Async.Parallel \[ task1; task2 \]} composes two task specifications generating a new value of type \texttt{Async<int\[\]>}
\item \texttt{Async.Run} takes this and runs it, returning the array \texttt{\[20; 40\]}. 
\end{itemize}


\subsection*{F\# 1.0 – Meta-programming}

tbd

\subsection*{F\# 1.0 – Improving the Functional Core: Indentation-aware Syntax}

tbd

\subsection*{F\# 1.0 – IDE Tooling}

tbd

\section*{Finance and Functional: Microsoft Commits to F\#, 2007}

tbd

\section*{F\# 2.0 – 2007 to 2010}

tbd

\section*{F\# 2.0 – Units of Measure }

tbd

\section*{Type Providers and F\# 3.0 }

tbd


\section*{.NET, F\# and the Shift to Cloud and Mobile Computing}

tbd


\section*{A New Dawn for F\#, C\# and .NET: Open and Cross-Platform, At Last!}

tbd

\section*{The F\# Community and the F\# Software Foundation }

tbd

\section*{.NET Core: Microsoft take C\#, F\# and .NET Cross-Platform}

tbd

\section*{F\# for Mobile}


tbd

\section*{F\#, JavaScript and Full Stack Programming}

tbd

\section*{Retrospective}

tbd


%% Acknowledgments
\begin{acks}                            %% acks environment is optional
                                        %% contents suppressed with 'anonymous'
acks
  %% Commands \grantsponsor{<sponsorID>}{<name>}{<url>} and
  %% \grantnum[<url>]{<sponsorID>}{<number>} should be used to
  %% acknowledge financial support and will be used by metadata
  %% extraction tools.
%  This material is based upon work supported by the
  %\grantsponsor{GS100000001}{National Science
    %Foundation}{http://dx.doi.org/10.13039/100000001} under Grant
  %No.~\grantnum{GS100000001}{nnnnnnn} and Grant
 % No.~\grantnum{GS100000001}{mmmmmmm}.  Any opinions, findings, and
  %conclusions or recommendations expressed in this material are those
 % of the author and do not necessarily reflect the views of the
  %National Science Foundation.
\end{acks}


%% Bibliography
%\bibliography{bibfile}


%% Appendix
\appendix
\section*{Appendix}

Text of appendix \ldots

\end{document}
